# -*- coding: utf-8 -*-
"""LinRegFS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MKNFHnsDcFMMpV8aLCUrKM4OoVfyn68Z

#File reading
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
import seaborn as sns
import matplotlib.pyplot as plt
#this is for baseline

import pandas as pd
from google.colab import drive

# 1. Mount Google Drive
# This will prompt you to click a link, sign in to your Google Account,
# and enter an authorization code.
drive.mount('/content/drive')

# 2. Define the path to your file
# IMPORTANT: You may need to adjust the path if the file is not directly in 'My Drive'.
# If you don't know the path, you can use the file browser in Colab to copy the path.
# Based on the screenshot, a likely path is:
file_path = '/content/drive/MyDrive/Housing.csv'

# 3. Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# 4. Display the first few rows to confirm it loaded correctly
print(df.head())

# 5. Display the column info
print(df.info())

categorical_colums = df.select_dtypes(include = ['object']).columns.tolist()
encoder = OneHotEncoder(sparse_output = False)
one_hot_encoded = encoder.fit_transform(df[categorical_colums])
one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_colums))

df_encoded = pd.concat([df, one_hot_df], axis=1)

df_encoded = df_encoded.drop(categorical_colums, axis=1)
df_encoded.to_csv('Housing_encoded.csv', index=False)

print(f"encoded data : \n{df_encoded}")

"""#using Libraries like linear, lasso, and ridge regression for a baseline"""

#@title Code - Load dependencies

# data
import numpy as np
import pandas as pd

# machine learning
import keras


# data visualization
import plotly.express as px

df.corr(numeric_only = True)

px.scatter_matrix(df, dimensions=["price", "area", "bathrooms"])

y = df_encoded['price']
X = df_encoded.drop('price', axis = 1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)

from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_train, y_train)

y_pred = reg.predict(X_test)

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
mae = mean_absolute_error(y_test, y_pred)
print(f"R-squared (R2) Score: {r2:.4f}")
print(f"rmse Score: {rmse:.4f}")
print(f"mae score : {mae:.4f}")

coef_df = pd.DataFrame({'feature': X.columns, 'Coefficient': reg.coef_})
coef_df['Absolute_coef'] = np.abs(coef_df['Coefficient'])
print("\nTop 10 most Important Features:")
print(coef_df.sort_values(by = 'Absolute_coef', ascending = False).head(10))
print(f"\nModel Intercept(Bias): ${reg.intercept_:.3f}")

"""Model bad try again"""

from sklearn.preprocessing import StandardScaler
numerical_cols = [
    'bathrooms',
    'stories',
    'airconditioning_yes',
    'airconditioning_no',
    'hotwaterheating_yes',
    'hotwaterheating_no',
    'parking',
    'prefarea_no',
    'prefarea_yes',
    'basement_yes'
]
X_train_numerical = X_train[numerical_cols]
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_numerical)
X_test_scaled = scaler.transform(X_test[numerical_cols])

X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=numerical_cols)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=numerical_cols)

# 5. Drop the original unscaled columns from the main X_train and X_test DataFrames
X_train_dropped = X_train.drop(numerical_cols, axis=1)
X_test_dropped = X_test.drop(numerical_cols, axis=1)

X_train_dropped = X_train_dropped.reset_index(drop = True)
X_test_dropped = X_test_dropped.reset_index(drop = True)

X_train_final = pd.concat([X_train_dropped, X_train_scaled_df], axis =1)
X_test_final = pd.concat([X_test_dropped, X_test_scaled_df], axis =1)

print("Final Training Data Shape:", X_train_final.shape)
print("Final Test Data Shape:", X_test_final.shape)

reg2 = LinearRegression()
reg2.fit(X_train_final, y_train)

y_pred2 = reg2.predict(X_test_final)

"""trying ridge and lasso"""

from sklearn.linear_model import Ridge, Lasso
ridge_model = Ridge(alpha = 500)
ridge_model.fit(X_train_final, y_train)

ridge_pred = ridge_model.predict(X_test_final)
ridge_r2 = r2_score(y_test, ridge_pred)
ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))

print(f"--- Ridge Regression ---")
print(f"R-squared (R2) Score: {ridge_r2:.4f}")
print(f"RMSE Score: {ridge_rmse:.4f}")

lasso_model = Lasso(alpha = 500)
lasso_model.fit(X_train_final, y_train)

lasso_pred = lasso_model.predict(X_test_final)
lasso_r2 = r2_score(y_test, lasso_pred)
lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_pred))


print(f"--- Lasso Regression ---")
print(f"R-squared (R2) Score: {lasso_r2:.4f}")
print(f"RMSE Score: {lasso_rmse:.4f}")

"""Cross Validation"""

from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

ALPHAS = np.logspace(-4, 4, 10)

# --- Ridge Regression with Cross-Validation (RidgeCV) ---

# 'cv=10' means 10-fold cross-validation will be used on the training data.
ridge_cv_model = RidgeCV(alphas=ALPHAS, cv=10, scoring='neg_mean_squared_error')


ridge_cv_model.fit(X_train_final, y_train)

best_alpha_ridge = ridge_cv_model.alpha_

ridge_cv_pred = ridge_cv_model.predict(X_test_final)
ridge_cv_r2 = r2_score(y_test, ridge_cv_pred)

print(f"--- RidgeCV Results ---")
print(f"Optimal Alpha (λ): {best_alpha_ridge:.4f}")
print(f"R-squared (R2) Score: {ridge_cv_r2:.4f}")

# --- Lasso Regression with Cross-Validation (LassoCV) ---


lasso_cv_model = LassoCV(alphas=ALPHAS, cv=10, random_state=42)

lasso_cv_model.fit(X_train_final, y_train)

best_alpha_lasso = lasso_cv_model.alpha_

# 8. Predict and evaluate using the model with the best alpha
lasso_cv_pred = lasso_cv_model.predict(X_test_final)
lasso_cv_r2 = r2_score(y_test, lasso_cv_pred)

print(f"\n--- LassoCV Results ---")
print(f"Optimal Alpha (λ): {best_alpha_lasso:.4f}")
print(f"R-squared (R2) Score: {lasso_cv_r2:.4f}")

"""#From Scratch"""

def loss_function(m, b, points):
    total_error = 0
    N = len(points)

# In both loss_function and gradient_descent_step:
    FEATURE_COL = 'area_scaled'
    TARGET_COL = 'price_scaled'

    for i in range(N):
        x = points.iloc[i][FEATURE_COL]
        y = points.iloc[i][TARGET_COL]

        # Predicted y: y_hat = m*x + b
        predicted_y = m * x + b

        # Squared error
        total_error += (y - predicted_y) ** 2

    # Mean Squared Error (MSE)
    return total_error / N

def gradient_descent_step(m, b, points, learning_rate):
    m_gradient = 0
    b_gradient = 0
    N = len(points)

    # Feature (X) column
    FEATURE_COL = 'area_scaled'
    # Target (Y) column
    TARGET_COL = 'price_scaled'

    for i in range(N):
        x = points.iloc[i][FEATURE_COL]
        y = points.iloc[i][TARGET_COL]

        # Error = Actual y - Predicted y
        error = y - (m * x + b)

        # Gradient for m: - (2/N) * sum(x * error)
        m_gradient += - (2/N) * x * error

        # Gradient for b: - (2/N) * sum(error)
        b_gradient += - (2/N) * error

    # Update m and b
    new_m = m - (learning_rate * m_gradient)
    new_b = b - (learning_rate * b_gradient)

    return new_m, new_b

def calculate_mean(data):
    # Mean (mu) = Sum of data / N
    return sum(data) / len(data)

def calculate_std_dev(data, mean):
    # Standard Deviation (sigma) = sqrt( Sum of (x - mu)^2 / N )
    N = len(data)
    variance = sum([(x - mean) ** 2 for x in data]) / N
    return variance ** 0.5 # Square root of the variance

def scale_data(data, mean, std_dev):
    # X_scaled = (X - mu) / sigma
    return [(x - mean) / std_dev for x in data]

# Using a copy so we don't accidentally modify the original data frame
data_scaled = df_encoded.copy()

# A. SCALE THE FEATURE: 'area'
area = data_scaled['area']

# 1. Calculate parameters
mean_area = calculate_mean(area)
std_dev_area = calculate_std_dev(area, mean_area)

print(f"Original Area Mean: {mean_area:.2f}, Std Dev: {std_dev_area:.2f}")

# 2. Apply scaling
data_scaled['area_scaled'] = scale_data(area, mean_area, std_dev_area)


#  B. SCALE THE TARGET: 'price'
price = data_scaled['price']

# 1. Calculate parameters
mean_price = calculate_mean(price)
std_dev_price = calculate_std_dev(price, mean_price)

print(f"Original Price Mean: {mean_price:.2f}, Std Dev: {std_dev_price:.2f}")

# 2. Apply scaling
data_scaled['price_scaled'] = scale_data(price, mean_price, std_dev_price)

print("\nScaled Data Head:")
print(data_scaled[['area', 'area_scaled', 'price', 'price_scaled']].head())

def gradient_descent_runner(points, starting_m, starting_b, learning_rate, num_iterations):
    m = starting_m
    b = starting_b

    print("Starting gradient descent at Loss = {0:.4f}".format(loss_function(m, b, points)))
    print("Running for {0} iterations...".format(num_iterations))

    for i in range(num_iterations):
        # The core step: update m and b
        m, b = gradient_descent_step(m, b, points, learning_rate)

        # Monitor progress
        if i % 100 == 0:
            loss = loss_function(m, b, points)
            print(f"Iteration {i}: Loss = {loss:.4f}")

    print("Finished.")
    return m, b

# UPDATED PARAMETERS FOR SCALED DATA
# Use the data_scaled DataFrame
data = data_scaled

# Initial parameters
initial_m = 0
initial_b = 0

# A faster learning rate is now possible because the data is scaled
learning_rate = 0.01
num_iterations = 1000

# Run the model with scaled data
final_m_scaled, final_b_scaled = gradient_descent_runner(
    data, initial_m, initial_b, learning_rate, num_iterations
)

print(f"\nScaled Optimal m (Slope for area_scaled): {final_m_scaled:.4f}")
print(f"Scaled Optimal b (Intercept/Bias): {final_b_scaled:.4f}")
print(f"Final Scaled Loss (MSE): {loss_function(final_m_scaled, final_b_scaled, data):.4f}")